{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Event Data for Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://datascience.stackexchange.com/questions/6547/open-source-anomaly-detection-in-python\n",
    "\n",
    "http://machinelearningmastery.com/time-series-data-visualization-with-python/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from azure.storage.table import TableService, Entity\n",
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadFromAzure = False\n",
    "\n",
    "#accountName = 'serviceshubproduswlog'\n",
    "#dateFloor = '20170101'\n",
    "#eventIds = np.array(['108']) #registraion reminder = 108\n",
    "#tableName = 'PremierHubInformation'\n",
    "\n",
    "accountName = 'profileserviceproduswlog'\n",
    "dateFloor = '20160101'\n",
    "eventIds = np.array(['89']) #support contact\n",
    "tableName = 'SironaInformation'\n",
    "\n",
    "current_dir = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads event data from Azure table storage.\n",
    "Event data is spanned across multiple tables based on severity.  Since we are only analyzing one event at a time, we \n",
    "must indicate the tableName and eventId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getEventIds(accountName, key, tableName, dateFloor, eventId):\n",
    "    \n",
    "    table_service = TableService(account_name=accountName, account_key=key)\n",
    "    \n",
    "    query = \"PartitionKey ge '\" + dateFloor + \"' and Level eq 8 and EventId eq \" + eventId\n",
    "     \n",
    "    entities=table_service.query_entities(tableName,\n",
    "                                          query, \n",
    "                                          select=\"PartitionKey, EventId\")\n",
    "    print ('Writing to ' + fileName)\n",
    "    \n",
    "    count = 0\n",
    "    with open(fileName, 'w+') as csvfile:\n",
    "        eventWriter = csv.writer(csvfile, delimiter=' ',\n",
    "                        quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for entity in entities:\n",
    "            count+=1\n",
    "            eventWriter.writerow([entity.PartitionKey, entity.EventId.value])\n",
    "            if count % 100 == 1:\n",
    "                print('.')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "    print()\n",
    "    print ('Completed Writing ' + str(count) + ' lines to ' + fileName) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if loadFromAzure:\n",
    "    for eventId in eventIds:\n",
    "        fileName =  current_dir + '/' + tableName + '_eventId_' + eventId + '.csv'\n",
    "        getEventIds(accountName, getpass.getpass(), tableName, dateFloor, eventId)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the top few imported rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PartitionKey EventId\n",
      "0     20161222      89\n",
      "1     20161227      89\n",
      "2     20161227      89\n",
      "3     20161227      89\n",
      "4     20161227      89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "tableNameToAnalyze = tableName\n",
    "eventIdToAnalyze = eventIds[0]\n",
    "fileName = current_dir + '/' + tableNameToAnalyze + '_eventId_' + eventIdToAnalyze + '.csv'\n",
    "\n",
    "events = pd.read_csv(fileName, delim_whitespace=True, dtype={'PartitionKey': object, 'EventId': object}, names = [\"PartitionKey\", \"EventId\"])\n",
    "print(events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate date range for every day since first event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + datetime.timedelta(n)\n",
    "\n",
    "end_date = datetime.date.today()\n",
    "start = events['PartitionKey'][0]\n",
    "start_date = datetime.date(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n",
    "\n",
    "partition_range = []\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    partition_range.append(single_date.strftime('%Y%m%d')) \n",
    "\n",
    "partition_array = np.zeros((len(partition_range), 2), dtype=object)\n",
    "\n",
    "eventsByDay = events.groupby(['PartitionKey'])\n",
    "\n",
    "partition_array[:,0] = partition_range[:]\n",
    "\n",
    "for x, y in eventsByDay.PartitionKey:\n",
    "    partition_array[np.where(partition_array==x)[0],1] = y.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moving_avg = movingaverage(partition_array[:,1], 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection (Statistical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-129-1be28435784f>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-129-1be28435784f>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    anomalies.append([[np.where(partition_array==partition_array[ii, 0])[0][0]], [partition_array[ii, 1]])\u001b[0m\n\u001b[0m                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num_events, num_cols = partition_array.shape\n",
    "\n",
    "std_moving_avg = np.std(moving_avg)\n",
    "anomalies= [[],[]]\n",
    "#print(np.where(partition_array=='20170216')[0][0])\n",
    "#print(np.where(partition_array=='20170217'))\n",
    "\n",
    "for ii in range(i):\n",
    "    if partition_array[ii, 1] > moving_avg[ii]+ (std_moving_avg * 2):\n",
    "        anomalies.append([[np.where(partition_array==partition_array[ii, 0])[0][0]], [partition_array[ii, 1]])\n",
    "        #anomalies[ii].append(np.where(partition_array==partition_array[ii, 0])[0][0])\n",
    "        #anomalies[ii].append(partition_array[ii, 1])\n",
    "        \n",
    "print (anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-fcef9d782ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomalies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'k.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not tuple"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set chart size\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 9\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.title('Number of EventId ' + eventIdToAnalyze + ' by day with moving average')\n",
    "plt.ylabel('events')\n",
    "plt.xlabel('day')\n",
    "\n",
    "plt.plot(partition_array[:,1])\n",
    "plt.plot(moving_avg, 'r')\n",
    "plt.plot(anomalies[:,1] , 'k.')\n",
    "step_size = 5\n",
    "plt.xticks([p for p in range(0, num_events, step_size)], partition_array[0::step_size, 0])\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=-60)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
